diff --git a/configs/ppo/config.yaml b/configs/ppo/config.yaml
index e65dc5d..3a5d2a8 100644
--- a/configs/ppo/config.yaml
+++ b/configs/ppo/config.yaml
@@ -13,7 +13,7 @@ cptdir: ""
 headless: True
 
 defaults:
-  - task: FrankaPick
+  - task: FrankaPickObject
   - train: ${task}
   - hydra/job_logging: disabled
 
diff --git a/configs/ppo/train/FrankaPickObject.yaml b/configs/ppo/train/FrankaPickObject.yaml
index 0029886..cd016ec 100644
--- a/configs/ppo/train/FrankaPickObject.yaml
+++ b/configs/ppo/train/FrankaPickObject.yaml
@@ -12,7 +12,7 @@ learn:
   save_interval: 50
   print_log: True
 
-  max_iterations: 2000
+  max_iterations: 50
 
   cliprange: 0.1
   ent_coef: 0
diff --git a/configs/ppo/train/FrankaPickObjectPixels.yaml b/configs/ppo/train/FrankaPickObjectPixels.yaml
index 1365993..413dbce 100644
--- a/configs/ppo/train/FrankaPickObjectPixels.yaml
+++ b/configs/ppo/train/FrankaPickObjectPixels.yaml
@@ -3,7 +3,7 @@ torch_deterministic: False
 
 encoder:
   name: vits-mae-hoi
-  pretrain_dir: "/tmp/pretrained"
+  pretrain_dir: none
   freeze: True
   emb_dim: 128
 
@@ -18,7 +18,7 @@ learn:
   save_interval: 50
   print_log: True
 
-  max_iterations: 2000
+  max_iterations: 10
 
   cliprange: 0.1
   ent_coef: 0
diff --git a/mvp/ppo/ppo.py b/mvp/ppo/ppo.py
index 013b96b..ecd8d38 100644
--- a/mvp/ppo/ppo.py
+++ b/mvp/ppo/ppo.py
@@ -18,6 +18,8 @@ from torch.utils.tensorboard import SummaryWriter
 
 from mvp.ppo import RolloutStorage
 
+import wandb
+
 
 class PPO:
 
diff --git a/tools/train_ppo.py b/tools/train_ppo.py
index 2f197c1..559a05b 100644
--- a/tools/train_ppo.py
+++ b/tools/train_ppo.py
@@ -11,9 +11,13 @@ from mvp.utils.hydra_utils import set_np_formatting, set_seed
 from mvp.utils.hydra_utils import parse_sim_params, parse_task
 from mvp.utils.hydra_utils import process_ppo
 
+import wandb
+
 
 @hydra.main(config_name="config", config_path="../configs/ppo")
 def train(cfg: omegaconf.DictConfig):
+    
+    wandb.init(project='mvp', entity='chengtianyue', sync_tensorboard=True)
 
     # Assume no multi-gpu training
     assert cfg.num_gpus == 1
@@ -39,6 +43,6 @@ def train(cfg: omegaconf.DictConfig):
     ppo = process_ppo(env, cfg, cfg_dict, cfg.logdir, cfg.cptdir)
     ppo.run(num_learning_iterations=cfg.train.learn.max_iterations, log_interval=cfg.train.learn.save_interval)
 
-
+    wandb.finish()
 if __name__ == '__main__':
     train()
